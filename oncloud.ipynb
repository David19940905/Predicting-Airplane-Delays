{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Predicting Airplane Delays\n",
    "\n",
    "The goals of this notebook are:\n",
    "- Process and create a dataset from downloaded ZIP files\n",
    "- Exploratory data analysis (EDA)\n",
    "- Establish a baseline model and improve it\n",
    "\n",
    "## Introduction to business scenario\n",
    "You work for a travel booking website that is working to improve the customer experience for flights that were delayed. The company wants to create a feature to let customers know if the flight will be delayed due to weather when the customers are booking the flight to or from the busiest airports for domestic travel in the US. \n",
    "\n",
    "You are tasked with solving part of this problem by leveraging machine learning to identify whether the flight will be delayed due to weather. You have been given access to the a dataset of on-time performance of domestic flights operated by large air carriers. You can use this data to train a machine learning model to predict if the flight is going to be delayed for the busiest airports.\n",
    "\n",
    "### Dataset\n",
    "The provided dataset contains scheduled and actual departure and arrival times reported by certified US air carriers that account for at least 1 percent of domestic scheduled passenger revenues. The data was collected by the Office of Airline Information, Bureau of Transportation Statistics (BTS). The dataset contains date, time, origin, destination, airline, distance, and delay status of flights for flights between 2014 and 2018.\n",
    "The data are in 60 compressed files, where each file contains a CSV for the flight details in a month for the five years (from 2014 - 2018). The data can be downloaded from this [link](https://ucstaff-my.sharepoint.com/:f:/g/personal/ibrahim_radwan_canberra_edu_au/EhWeqeQsh-9Mr1fneZc9_0sBOBzEdXngvxFJtAlIa-eAgA?e=8ukWwa). Please download the data files and place them on a relative path. Dataset(s) used in this assignment were compiled by the Office of Airline Information, Bureau of Transportation Statistics (BTS), Airline On-Time Performance Data, available with the following [link](https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare the environment \n",
    "\n",
    "Use one of the labs which we have practised on with the Amazon Sagemakers where you perform the following steps:\n",
    "1. Start a lab.\n",
    "2. Create a notebook instance and name it \"oncloudproject\".\n",
    "3. Increase the used memory to 25 GB from the additional configurations.\n",
    "4. Open Jupyter Lab and upload this notebook into it.\n",
    "5. Upload the two combined CVS files (combined_csv_v1.csv and combined_csv_v2.csv), which you created in Part A of this project.\n",
    "\n",
    "**Note:** In case of the data is too much to be uploaded to the AWS, please use 20% of the data only for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Build and evaluate simple models\n",
    "\n",
    "Write code to perform the follwoing steps:\n",
    "1. Split data into training, validation and testing sets (70% - 15% - 15%).\n",
    "2. Use linear learner estimator to build a classifcation model.\n",
    "3. Host the model on another instance\n",
    "4. Perform batch transform to evaluate the model on testing data\n",
    "5. Report the performance metrics that you see better test the model performance \n",
    "\n",
    "Note: You are required to perform the above steps on the two combined datasets separatey and to comments on the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import tarfile\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "# SageMaker\n",
    "import sagemaker\n",
    "from sagemaker import Session, image_uris\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n",
    "from sagemaker.amazon.linear_learner import LinearLearner\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker and AWS region\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Load dataset\n",
    "CSV_PATH = \"combined_csv.csv\"\n",
    "RANDOM_STATE = 42\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Check missing values\n",
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "missing_cols = missing_counts[missing_counts > 0]\n",
    "missing_total_before = int(df.isna().sum().sum())\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_clean = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Identify column types\n",
    "numeric_columns = df_clean.select_dtypes(include=\"number\").columns.tolist()\n",
    "non_numeric_columns = df_clean.select_dtypes(exclude=\"number\").columns.tolist()\n",
    "\n",
    "# Find target column\n",
    "if \"target\" in df_clean.columns:\n",
    "    target_col = \"target\"\n",
    "elif \"is_delay\" in df_clean.columns:\n",
    "    target_col = \"is_delay\"\n",
    "else:\n",
    "    target_col = None\n",
    "\n",
    "# Check if stratified sampling can be used\n",
    "stratified = (\n",
    "    target_col is not None\n",
    "    and 2 <= df_clean[target_col].nunique() <= 20\n",
    ")\n",
    "\n",
    "# Check missing values again after cleaning\n",
    "missing_total_after = int(df_clean.isna().sum().sum())\n",
    "\n",
    "# Summary\n",
    "results = {\n",
    "    \"missing_counts\": missing_counts,\n",
    "    \"missing_columns\": missing_cols,\n",
    "    \"total_missing_before\": missing_total_before,\n",
    "    \"total_missing_after\": missing_total_after,\n",
    "    \"numeric_columns\": numeric_columns,\n",
    "    \"non_numeric_columns\": non_numeric_columns,\n",
    "    \"target_column\": target_col,\n",
    "    \"stratified\": stratified,\n",
    "    \"cleaned_shape\": df_clean.shape,\n",
    "}\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Choose cleaned dataset if it exists\n",
    "data = df_clean if \"df_clean\" in globals() else df\n",
    "\n",
    "# Pick stratify column if valid\n",
    "if \"target_col\" in globals() and target_col in data.columns and 2 <= data[target_col].nunique() <= 20:\n",
    "    stratify_col = data[target_col]\n",
    "else:\n",
    "    stratify_col = None\n",
    "\n",
    "# 70% train / 30% temp split\n",
    "train_df, temp_df = train_test_split(\n",
    "    data,\n",
    "    test_size=0.3,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=stratify_col\n",
    ")\n",
    "\n",
    "# Stratify again for validation/test split\n",
    "if \"target_col\" in globals() and target_col in temp_df.columns and 2 <= temp_df[target_col].nunique() <= 20:\n",
    "    strat_temp = temp_df[target_col]\n",
    "else:\n",
    "    strat_temp = None\n",
    "\n",
    "# Split 30% temp into 15% validation and 15% test\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=strat_temp\n",
    ")\n",
    "\n",
    "# Save data splits\n",
    "os.makedirs(\"splits\", exist_ok=True)\n",
    "train_df.to_csv(\"splits/train.csv\", index=False)\n",
    "val_df.to_csv(\"splits/val.csv\", index=False)\n",
    "test_df.to_csv(\"splits/test.csv\", index=False)\n",
    "\n",
    "# Print summary\n",
    "def show_info(name, df, label):\n",
    "    print(f\"{name}: {df.shape[0]} rows × {df.shape[1]} cols\")\n",
    "    if label and label in df.columns:\n",
    "        counts = df[label].value_counts(dropna=False).sort_index()\n",
    "        percents = (counts / len(df) * 100).round(2)\n",
    "        dist = \", \".join([f\"{i}: {counts[i]} ({percents[i]}%)\" for i in counts.index])\n",
    "        print(\"class distribution ->\", dist)\n",
    "    print()\n",
    "\n",
    "print(\"Splits saved in ./splits/\")\n",
    "show_info(\"Train 70%\", train_df, target_col if \"target_col\" in globals() else None)\n",
    "show_info(\"Validation 15%\", val_df, target_col if \"target_col\" in globals() else None)\n",
    "show_info(\"Test 15%\", test_df, target_col if \"target_col\" in globals() else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pathlib import Path\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Local files\n",
    "DATA_SPLITS = {\n",
    "    \"train\": Path(\"splits/train.csv\"),\n",
    "    \"val\":   Path(\"splits/val.csv\"),\n",
    "    \"test\":  Path(\"splits/test.csv\"),\n",
    "}\n",
    "\n",
    "# S3 config\n",
    "BUCKET_NAME = \"c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft\"\n",
    "KEY_PREFIX  = \"linear-learner-delay/\"\n",
    "\n",
    "# S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def upload_file_checked(local_file: Path, bucket: str, key: str):\n",
    "    \"\"\"Upload to S3 and confirm object exists.\"\"\"\n",
    "    if not local_file.exists():\n",
    "        print(f\"skip (not found): {local_file}\")\n",
    "        return\n",
    "    try:\n",
    "        s3.upload_file(str(local_file), bucket, key)\n",
    "        s3.head_object(Bucket=bucket, Key=key)\n",
    "        print(f\"ok: s3://{bucket}/{key}\")\n",
    "    except ClientError as e:\n",
    "        msg = e.response.get(\"Error\", {}).get(\"Message\", str(e))\n",
    "        print(f\"error: {key} -> {msg}\")\n",
    "\n",
    "# Upload all splits\n",
    "for name, path in DATA_SPLITS.items():\n",
    "    key = f\"{KEY_PREFIX}{path.name}\"\n",
    "    upload_file_checked(path, BUCKET_NAME, key)\n",
    "\n",
    "# Print S3 URIs\n",
    "print(\"\\nS3 URIs:\")\n",
    "for name in (\"train\", \"val\", \"test\"):\n",
    "    print(f\"s3://{BUCKET_NAME}/{KEY_PREFIX}{name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Initialize SageMaker session\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# Get AWS region\n",
    "region = session.boto_region_name\n",
    "\n",
    "# Get execution role\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# S3 dataset channels\n",
    "train_s3 = f\"s3://{bucket}/{prefix}train.csv\"\n",
    "val_s3   = f\"s3://{bucket}/{prefix}val.csv\"\n",
    "test_s3  = f\"s3://{bucket}/{prefix}test.csv\"\n",
    "\n",
    "# SageMaker inputs (no header expected after sanitize)\n",
    "train_input = TrainingInput(train_s3, content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(val_s3,   content_type=\"text/csv\")\n",
    "\n",
    "# Info\n",
    "print(\"region:\", region)\n",
    "print(\"role:\", role)\n",
    "print(train_s3, val_s3, test_s3, sep=\"\\n\")\n",
    "\n",
    "# S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def read_csv_from_s3(key: str) -> pd.DataFrame:\n",
    "    body = s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read()\n",
    "    return pd.read_csv(io.BytesIO(body))\n",
    "\n",
    "def drop_suffix_dot1(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[[c for c in df.columns if not c.endswith(\".1\")]]\n",
    "\n",
    "def cast_bool_like(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == bool:\n",
    "            df[c] = df[c].astype(int)\n",
    "        elif df[c].dtype == object:\n",
    "            uniq = set(df[c].dropna().astype(str).str.lower().unique())\n",
    "            if uniq <= {\"true\", \"false\"}:\n",
    "                df[c] = df[c].astype(str).str.lower().map({\"true\": 1, \"false\": 0})\n",
    "    return df\n",
    "\n",
    "def coerce_all_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.apply(lambda s: pd.to_numeric(s, errors=\"coerce\"))\n",
    "\n",
    "def put_first(df: pd.DataFrame, label: str = \"target\") -> pd.DataFrame:\n",
    "    if label in df.columns:\n",
    "        cols = [label] + [c for c in df.columns if c != label]\n",
    "        return df[cols]\n",
    "    return df\n",
    "\n",
    "# ---------- main ----------\n",
    "def sanitize_one(name: str) -> None:\n",
    "    \"\"\"\n",
    "    Load s3://{bucket}/{prefix}{name}.csv (with header), clean, ensure binary label in col 0,\n",
    "    write back to the same key without header.\n",
    "    \"\"\"\n",
    "    key = f\"{prefix}{name}.csv\"\n",
    "\n",
    "    # load and clean\n",
    "    df = read_csv_from_s3(key)\n",
    "    df = drop_suffix_dot1(df)\n",
    "    df = cast_bool_like(df)\n",
    "    df = coerce_all_numeric(df).fillna(0.0)\n",
    "    df = put_first(df, \"target\")\n",
    "\n",
    "    # ensure label is {0,1}\n",
    "    y = df.iloc[:, 0].to_numpy()\n",
    "    uniq = np.unique(y[~pd.isna(y)])\n",
    "    if not set(uniq) <= {0, 1}:\n",
    "        if len(uniq) == 2:\n",
    "            lo, hi = sorted(uniq)\n",
    "            df.iloc[:, 0] = (df.iloc[:, 0] == hi).astype(int)\n",
    "        else:\n",
    "            thr = np.nanmedian(y)\n",
    "            df.iloc[:, 0] = (df.iloc[:, 0] > thr).astype(int)\n",
    "\n",
    "    # write back (no header/index)\n",
    "    buf = io.StringIO()\n",
    "    df.to_csv(buf, header=False, index=False)\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=buf.getvalue(), ContentType=\"text/csv\")\n",
    "\n",
    "    # verify\n",
    "    s3.head_object(Bucket=bucket, Key=key)\n",
    "    print(f\"sanitized: s3://{bucket}/{key}  shape={df.shape}\")\n",
    "\n",
    "def sanitize_all():\n",
    "    for part in (\"train\", \"val\", \"test\"):\n",
    "        sanitize_one(part)\n",
    "\n",
    "# run:\n",
    "# sanitize_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image URI for Linear Learner\n",
    "img_uri = image_uris.retrieve(region=region, framework=\"linear-learner\")\n",
    "\n",
    "# Estimator configuration\n",
    "output_url = f\"s3://{bucket}/{prefix}output/\"\n",
    "instance_type = \"ml.m5.large\"\n",
    "\n",
    "ll_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=img_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=output_url,\n",
    "    sagemaker_session=sess,\n",
    "    max_run=2000,\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "hp = {\n",
    "    \"predictor_type\": \"binary_classifier\",\n",
    "    \"epochs\": 5,\n",
    "    \"mini_batch_size\": 256,\n",
    "    \"num_models\": 32,\n",
    "    \"loss\": \"auto\",\n",
    "}\n",
    "ll_estimator.set_hyperparameters(**hp)\n",
    "\n",
    "# Data channels\n",
    "train_channel = TrainingInput(f\"s3://{bucket}/{prefix}train.csv\", content_type=\"text/csv\")\n",
    "valid_channel = TrainingInput(f\"s3://{bucket}/{prefix}val.csv\",   content_type=\"text/csv\")\n",
    "channels = {\"train\": train_channel, \"validation\": valid_channel}\n",
    "\n",
    "# Fit\n",
    "ll_estimator.fit(channels, logs=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, time\n",
    "from pathlib import Path\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "# deploy\n",
    "ep_name = f\"ll-{int(time.time())}\"\n",
    "pred = est.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=ep_name)\n",
    "print(\"endpoint:\", pred.endpoint_name)\n",
    "\n",
    "# load test set\n",
    "resp = s3.get_object(Bucket=bucket, Key=f\"{prefix}test.csv\")\n",
    "df_test = pd.read_csv(io.BytesIO(resp[\"Body\"].read()), header=None)\n",
    "y_true = df_test.iloc[:, 0].astype(\"int32\").to_numpy()\n",
    "X_test = df_test.iloc[:, 1:]\n",
    "\n",
    "# upload features-only (in-memory)\n",
    "csv_buf = io.StringIO()\n",
    "X_test.to_csv(csv_buf, header=False, index=False)\n",
    "csv_bytes = io.BytesIO(csv_buf.getvalue().encode(\"utf-8\"))\n",
    "test_x_key = f\"{prefix}test_x.csv\"\n",
    "s3.put_object(Bucket=bucket, Key=test_x_key, Body=csv_bytes.getvalue())\n",
    "test_x_uri = f\"s3://{bucket}/{test_x_key}\"\n",
    "\n",
    "# batch transform\n",
    "xformer = est.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/{prefix}batch_out/\",\n",
    "    accept=\"application/jsonlines\",\n",
    "    assemble_with=\"Line\",\n",
    ")\n",
    "\n",
    "xformer.transform(data=test_x_uri, content_type=\"text/csv\", split_type=\"Line\")\n",
    "xformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# ---- Load Batch Transform output files ----\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "bucket_obj = s3_resource.Bucket(bucket)\n",
    "out_keys = [\n",
    "    obj.key\n",
    "    for obj in bucket_obj.objects.filter(Prefix=f\"{prefix}batch_out/\")\n",
    "    if obj.key.endswith(\".out\")\n",
    "]\n",
    "\n",
    "# ---- Extract predictions ----\n",
    "probs, preds = [], []\n",
    "for k in out_keys:\n",
    "    text = bucket_obj.Object(k).get()[\"Body\"].read().decode(\"utf-8\")\n",
    "    if not text.strip():\n",
    "        continue\n",
    "    for line in text.splitlines():\n",
    "        rec = json.loads(line)\n",
    "        p = rec.get(\"score\", rec.get(\"scores\", [0])[0])\n",
    "        lbl = rec.get(\"predicted_label\", int(p >= 0.5))\n",
    "        probs.append(float(p))\n",
    "        preds.append(int(lbl))\n",
    "\n",
    "y_prob = np.array(probs, dtype=float)\n",
    "y_pred = np.array(preds, dtype=int)\n",
    "\n",
    "if len(y_true) != len(y_pred):\n",
    "    raise ValueError(\"Length mismatch between true and predicted values\")\n",
    "\n",
    "# ---- Calculate metrics ----\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "auc = roc_auc_score(y_true, y_prob)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# ---- Print results ----\n",
    "print(\"Batch Transform - Evaluation Metrics\")\n",
    "print(f\"Accuracy     : {acc:.4f}\")\n",
    "print(f\"Precision    : {prec:.4f}\")\n",
    "print(f\"Recall       : {rec:.4f}\")\n",
    "print(f\"F1-score     : {f1:.4f}\")\n",
    "print(f\"ROC AUC      : {auc:.4f}\")\n",
    "print(\"Confusion matrix [[TN FP]\\n [FN TP]]:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Build and evaluate ensembe models\n",
    "\n",
    "Write code to perform the follwoing steps:\n",
    "1. Split data into training, validation and testing sets (70% - 15% - 15%).\n",
    "2. Use xgboost estimator to build a classifcation model.\n",
    "3. Host the model on another instance\n",
    "4. Perform batch transform to evaluate the model on testing data\n",
    "5. Report the performance metrics that you see better test the model performance \n",
    "6. write down your observation on the difference between the performance of using the simple and ensemble models.\n",
    "Note: You are required to perform the above steps on the two combined datasets separatey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve XGBoost image\n",
    "xgb_image = image_uris.retrieve(framework=\"xgboost\", region=region, version=\"1.7-1\")\n",
    "\n",
    "# Define estimator configuration\n",
    "xgb_output_path = f\"s3://{bucket}/{prefix}xgb_results/\"\n",
    "xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=xgb_output_path,\n",
    "    sagemaker_session=sess,\n",
    "    max_run=2000,\n",
    ")\n",
    "\n",
    "# Set hyperparameters for binary classification\n",
    "xgb_hyperparams = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": 75,\n",
    "    \"max_depth\": 5,\n",
    "    \"eta\": 0.15,\n",
    "    \"subsample\": 0.7,\n",
    "    \"eval_metric\": \"auc\",\n",
    "}\n",
    "xgb_estimator.set_hyperparameters(**xgb_hyperparams)\n",
    "\n",
    "# Create data channels\n",
    "train_data = TrainingInput(f\"s3://{bucket}/{prefix}train.csv\", content_type=\"text/csv\")\n",
    "val_data   = TrainingInput(f\"s3://{bucket}/{prefix}val.csv\",   content_type=\"text/csv\")\n",
    "channels = {\"train\": train_data, \"validation\": val_data}\n",
    "\n",
    "# Start training\n",
    "xgb_estimator.fit(channels, logs=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "# ----- Deploy a realtime endpoint -----\n",
    "ep_name = f\"xgb-rt-{int(time.time())}\"\n",
    "xgb_predictor = xgb_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    endpoint_name=ep_name,\n",
    ")\n",
    "print(\"XGBoost endpoint:\", xgb_predictor.endpoint_name)\n",
    "\n",
    "# ----- Configure Batch Transform -----\n",
    "bt_output = f\"s3://{bucket}/{prefix}xgb_bt_out/\"\n",
    "xgb_bt = xgb_estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=bt_output,\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"application/jsonlines\",\n",
    ")\n",
    "\n",
    "# ----- Run Batch Transform on features-only test CSV -----\n",
    "test_x_uri = f\"s3://{bucket}/{prefix}test_x.csv\"\n",
    "xgb_bt.transform(data=test_x_uri, content_type=\"text/csv\", split_type=\"Line\")\n",
    "xgb_bt.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import re\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# ---------- S3 handles ----------\n",
    "s3_res = boto3.resource(\"s3\")\n",
    "s3_cli = boto3.client(\"s3\")\n",
    "bucket_obj = s3_res.Bucket(bucket)\n",
    "\n",
    "# Gather XGBoost batch outputs (deterministic order)\n",
    "bt_prefix = f\"{prefix}xgb_batch_out/\"\n",
    "shard_keys = sorted(k.key for k in bucket_obj.objects.filter(Prefix=bt_prefix) if k.key.endswith(\".out\"))\n",
    "\n",
    "def get_prob(line: str) -> float | None:\n",
    "    \"\"\"\n",
    "    Try to read a probability from a single output line.\n",
    "    Supports JSON (scalar/dict/list) and simple CSV/whitespace tokens.\n",
    "    Returns None if nothing numeric is found.\n",
    "    \"\"\"\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    # JSON path\n",
    "    try:\n",
    "        obj = json.loads(line)\n",
    "\n",
    "        # scalar number\n",
    "        if isinstance(obj, (int, float)):\n",
    "            return float(obj)\n",
    "\n",
    "        # dict with common fields, then first numeric fallback\n",
    "        if isinstance(obj, dict):\n",
    "            for key in (\"score\", \"probability\", \"prediction\", \"predicted_value\"):\n",
    "                val = obj.get(key)\n",
    "                if isinstance(val, (int, float)):\n",
    "                    return float(val)\n",
    "            for key in (\"scores\", \"predictions\"):\n",
    "                val = obj.get(key)\n",
    "                if isinstance(val, (list, tuple)) and val and isinstance(val[0], (int, float)):\n",
    "                    return float(val[0])\n",
    "            for val in obj.values():\n",
    "                if isinstance(val, (int, float)):\n",
    "                    return float(val)\n",
    "\n",
    "        # list: return first numeric\n",
    "        if isinstance(obj, list):\n",
    "            for val in obj:\n",
    "                if isinstance(val, (int, float)):\n",
    "                    return float(val)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # CSV / whitespace tokens\n",
    "    for tok in re.split(r\"[,\\s]+\", line):\n",
    "        if not tok:\n",
    "            continue\n",
    "        try:\n",
    "            return float(tok)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return None\n",
    "\n",
    "# ---------- Parse predictions ----------\n",
    "probs, preds = [], []\n",
    "for key in shard_keys:\n",
    "    text = bucket_obj.Object(key).get()[\"Body\"].read().decode(\"utf-8\")\n",
    "    if not text:\n",
    "        continue\n",
    "    for ln in text.splitlines():\n",
    "        p = get_prob(ln)\n",
    "        if p is None:\n",
    "            continue\n",
    "        probs.append(p)\n",
    "        preds.append(int(p >= 0.5))\n",
    "\n",
    "y_prob = np.asarray(probs, dtype=float)\n",
    "y_pred = np.asarray(preds, dtype=int)\n",
    "\n",
    "# ---------- Load ground truth (label is first column) ----------\n",
    "obj = s3_cli.get_object(Bucket=bucket, Key=f\"{prefix}test.csv\")\n",
    "test_df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()), header=None)\n",
    "y_true = test_df.iloc[:, 0].astype(int).to_numpy()\n",
    "\n",
    "# Align lengths if needed (common with header rows or partial shards)\n",
    "if len(y_true) != len(y_pred):\n",
    "    print(\n",
    "        f\"[notice] length mismatch: y_true={len(y_true)}, y_pred={len(y_pred)}. \"\n",
    "        \"Truncating to the shorter length.\"\n",
    "    )\n",
    "    m = min(len(y_true), len(y_pred))\n",
    "    y_true, y_pred, y_prob = y_true[:m], y_pred[:m], y_prob[:m]\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "# compute AUC only when labels contain both classes and probabilities vary\n",
    "if y_true.size > 0 and np.unique(y_true).size == 2 and np.std(y_prob) > 0:\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "else:\n",
    "    auc = float(\"nan\")\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# ---------- Report ----------\n",
    "print(\"XGBoost Batch Transform — Test Summary\")\n",
    "print(f\"Accuracy   : {acc:.4f}\")\n",
    "print(f\"Precision  : {prec:.4f}\")\n",
    "print(f\"Recall     : {rec:.4f}\")\n",
    "print(f\"F1-score   : {f1:.4f}\")\n",
    "print(f\"ROC AUC    : {auc:.4f}\")\n",
    "print(\"Confusion matrix [[TN FP]\\n [FN TP]]:\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2025.06-py3.11",
   "language": "python",
   "name": "conda-env-anaconda-2025.06-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
