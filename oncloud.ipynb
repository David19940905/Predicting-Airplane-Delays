{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Predicting Airplane Delays\n",
    "\n",
    "The goals of this notebook are:\n",
    "- Process and create a dataset from downloaded ZIP files\n",
    "- Exploratory data analysis (EDA)\n",
    "- Establish a baseline model and improve it\n",
    "\n",
    "## Introduction to business scenario\n",
    "You work for a travel booking website that is working to improve the customer experience for flights that were delayed. The company wants to create a feature to let customers know if the flight will be delayed due to weather when the customers are booking the flight to or from the busiest airports for domestic travel in the US. \n",
    "\n",
    "You are tasked with solving part of this problem by leveraging machine learning to identify whether the flight will be delayed due to weather. You have been given access to the a dataset of on-time performance of domestic flights operated by large air carriers. You can use this data to train a machine learning model to predict if the flight is going to be delayed for the busiest airports.\n",
    "\n",
    "### Dataset\n",
    "The provided dataset contains scheduled and actual departure and arrival times reported by certified US air carriers that account for at least 1 percent of domestic scheduled passenger revenues. The data was collected by the Office of Airline Information, Bureau of Transportation Statistics (BTS). The dataset contains date, time, origin, destination, airline, distance, and delay status of flights for flights between 2014 and 2018.\n",
    "The data are in 60 compressed files, where each file contains a CSV for the flight details in a month for the five years (from 2014 - 2018). The data can be downloaded from this [link](https://ucstaff-my.sharepoint.com/:f:/g/personal/ibrahim_radwan_canberra_edu_au/EhWeqeQsh-9Mr1fneZc9_0sBOBzEdXngvxFJtAlIa-eAgA?e=8ukWwa). Please download the data files and place them on a relative path. Dataset(s) used in this assignment were compiled by the Office of Airline Information, Bureau of Transportation Statistics (BTS), Airline On-Time Performance Data, available with the following [link](https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare the environment \n",
    "\n",
    "Use one of the labs which we have practised on with the Amazon Sagemakers where you perform the following steps:\n",
    "1. Start a lab.\n",
    "2. Create a notebook instance and name it \"oncloudproject\".\n",
    "3. Increase the used memory to 25 GB from the additional configurations.\n",
    "4. Open Jupyter Lab and upload this notebook into it.\n",
    "5. Upload the two combined CVS files (combined_csv_v1.csv and combined_csv_v2.csv), which you created in Part A of this project.\n",
    "\n",
    "**Note:** In case of the data is too much to be uploaded to the AWS, please use 20% of the data only for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Build and evaluate simple models\n",
    "\n",
    "Write code to perform the follwoing steps:\n",
    "1. Split data into training, validation and testing sets (70% - 15% - 15%).\n",
    "2. Use linear learner estimator to build a classifcation model.\n",
    "3. Host the model on another instance\n",
    "4. Perform batch transform to evaluate the model on testing data\n",
    "5. Report the performance metrics that you see better test the model performance \n",
    "\n",
    "Note: You are required to perform the above steps on the two combined datasets separatey and to comments on the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import tarfile\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "# SageMaker\n",
    "import sagemaker\n",
    "from sagemaker import Session, image_uris\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n",
    "from sagemaker.amazon.linear_learner import LinearLearner\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker and AWS region\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Load dataset\n",
    "CSV_PATH = \"combined_csv.csv\"\n",
    "RANDOM_STATE = 42\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# --- Check for missing values ---\n",
    "# Count NaNs in each column (sorted descending)\n",
    "nan_counts = df.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# Keep only columns that actually have missing values\n",
    "missing_columns = nan_counts[nan_counts > 0]\n",
    "\n",
    "# Total number of missing values in the dataset\n",
    "total_missing_before = df.isna().sum().sum()\n",
    "\n",
    "# --- Handle missing values ---\n",
    "# Drop all rows that contain any missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Identify numeric and non-numeric columns\n",
    "numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "non_numeric_cols = df.select_dtypes(exclude=\"number\").columns\n",
    "\n",
    "# --- Determine target column ---\n",
    "# Select target column if present ('target' or 'is_delay')\n",
    "target_col = \"target\" if \"target\" in df.columns else (\n",
    "    \"is_delay\" if \"is_delay\" in df.columns else None\n",
    ")\n",
    "\n",
    "# Use stratified sampling if target has a reasonable number of unique values\n",
    "if target_col is not None and 2 <= df[target_col].nunique() <= 20:\n",
    "    strat = df[target_col]\n",
    "else:\n",
    "    strat = None\n",
    "\n",
    "# --- Final check after dropping missing values ---\n",
    "total_missing_after = df.isna().sum().sum()\n",
    "\n",
    "# --- Optional: return results instead of printing ---\n",
    "results = {\n",
    "    \"nan_counts\": nan_counts,\n",
    "    \"missing_columns\": missing_columns,\n",
    "    \"total_missing_before\": total_missing_before,\n",
    "    \"total_missing_after\": total_missing_after,\n",
    "    \"numeric_columns\": list(numeric_cols),\n",
    "    \"non_numeric_columns\": list(non_numeric_cols),\n",
    "    \"target_column\": target_col,\n",
    "    \"stratified\": strat is not None,\n",
    "}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split dataset into train/val/test (70/15/15) ---\n",
    "\n",
    "# 70% train / 30% temp\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.30,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=strat,  # use stratified sampling if available\n",
    ")\n",
    "\n",
    "# Stratify for temp split (validation/test)\n",
    "if target_col is not None and 2 <= temp_df[target_col].nunique() <= 20:\n",
    "    strat_temp = temp_df[target_col]\n",
    "else:\n",
    "    strat_temp = None\n",
    "\n",
    "# Split remaining 30% → 15% validation, 15% test\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,  # half of 30% = 15%\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=strat_temp,\n",
    ")\n",
    "\n",
    "# --- Save splits ---\n",
    "os.makedirs(\"splits\", exist_ok=True)\n",
    "train_df.to_csv(\"splits/train.csv\", index=False)\n",
    "val_df.to_csv(\"splits/val.csv\", index=False)\n",
    "test_df.to_csv(\"splits/test.csv\", index=False)\n",
    "\n",
    "\n",
    "# --- Reporting function ---\n",
    "def describe(name, d, tgt):\n",
    "    \"\"\"Print dataset shape and target class distribution.\"\"\"\n",
    "    print(f\"{name}: {len(d)} rows, {d.shape[1]} cols\")\n",
    "    if tgt and tgt in d.columns:\n",
    "        vc = d[tgt].value_counts(dropna=False).sort_index()\n",
    "        pct = (vc / len(d) * 100).round(2)\n",
    "        dist = \", \".join([f\"{k}: {vc[k]} ({pct[k]}%)\" for k in vc.index])\n",
    "        print(\" class distribution ->\", dist)\n",
    "    print()\n",
    "\n",
    "\n",
    "# --- Output summary (same as original) ---\n",
    "target_col = \"target\"\n",
    "print(\"saved splits to ./splits/\")\n",
    "describe(\"train 70%\", train_df, target_col)\n",
    "describe(\"val 15%\", val_df, target_col)\n",
    "describe(\"test 15%\", test_df, target_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Ensure output directory exists ---\n",
    "os.makedirs(\"splits\", exist_ok=True)\n",
    "\n",
    "# --- Function: Move target column to the first position ---\n",
    "def put_label_first(df, label=\"target\"):\n",
    "    \"\"\"Return a DataFrame with the target label as the first column.\"\"\"\n",
    "    if label in df.columns:\n",
    "        ordered_cols = [label] + [col for col in df.columns if col != label]\n",
    "        return df[ordered_cols]\n",
    "    return df\n",
    "\n",
    "# --- Reorder columns ---\n",
    "train_df = put_label_first(train_df, target_col)\n",
    "val_df   = put_label_first(val_df, target_col)\n",
    "test_df  = put_label_first(test_df, target_col)\n",
    "\n",
    "# --- Save CSV splits (with headers, no index) ---\n",
    "train_df.to_csv(\"splits/train.csv\", index=False, header=True)\n",
    "val_df.to_csv(\"splits/val.csv\", index=False, header=True)\n",
    "test_df.to_csv(\"splits/test.csv\", index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pathlib\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# --- Local dataset paths ---\n",
    "paths = {\n",
    "    \"train\": \"splits/train.csv\",\n",
    "    \"val\":   \"splits/val.csv\",\n",
    "    \"test\":  \"splits/test.csv\",\n",
    "}\n",
    "\n",
    "# --- S3 configuration ---\n",
    "bucket = \"c182567a4701745l12017053t1w416916046524-labbucket-fd5b733ssgft\"\n",
    "prefix = \"linear-learner-delay/\"\n",
    "\n",
    "# --- Create S3 client ---\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# --- Upload function with verification ---\n",
    "def upload_and_verify(local_path, bucket, key):\n",
    "    \"\"\"Upload file to S3 and verify upload success.\"\"\"\n",
    "    try:\n",
    "        s3.upload_file(local_path, bucket, key)\n",
    "        s3.head_object(Bucket=bucket, Key=key)\n",
    "        print(f\"uploaded → s3://{bucket}/{key}\")\n",
    "    except ClientError as e:\n",
    "        print(f\"could not verify {key}: {e}\")\n",
    "\n",
    "# --- Upload each dataset split ---\n",
    "for name, local_path in paths.items():\n",
    "    s3_key = f\"{prefix}{pathlib.Path(local_path).name}\"\n",
    "    upload_and_verify(local_path, bucket, s3_key)\n",
    "\n",
    "# --- Print resulting S3 URIs (same as original output) ---\n",
    "print(\"\\nS3 URIs for dataset:\")\n",
    "print(f\"s3://{bucket}/{prefix}train.csv\")\n",
    "print(f\"s3://{bucket}/{prefix}val.csv\")\n",
    "print(f\"s3://{bucket}/{prefix}test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "# --- Initialize SageMaker session ---\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# --- Get current AWS region ---\n",
    "region = sess.boto_region_name\n",
    "\n",
    "# --- Retrieve execution role for the current SageMaker environment ---\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define S3 dataset channels for SageMaker ---\n",
    "train_s3 = f\"s3://{bucket}/{prefix}train.csv\"\n",
    "val_s3   = f\"s3://{bucket}/{prefix}val.csv\"\n",
    "test_s3  = f\"s3://{bucket}/{prefix}test.csv\"\n",
    "\n",
    "# Training inputs (header-less expected later after sanitization)\n",
    "train_input = TrainingInput(train_s3, content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(val_s3,   content_type=\"text/csv\")\n",
    "\n",
    "# --- Console info (kept exactly as original) ---\n",
    "print(\"region:\", region)\n",
    "print(\"role:\", role)\n",
    "print(train_s3, val_s3, test_s3, sep=\"\\n\")\n",
    "\n",
    "# --- S3 client ---\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def load_csv_with_header(key: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a CSV object from S3 into a pandas DataFrame (assumes a header exists).\"\"\"\n",
    "    body = s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read()\n",
    "    return pd.read_csv(io.BytesIO(body))\n",
    "\n",
    "\n",
    "def drop_dot1(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicate columns that end with '.1'.\"\"\"\n",
    "    return df[[c for c in df.columns if not c.endswith(\".1\")]]\n",
    "\n",
    "\n",
    "def bool_to_int(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert boolean columns to int; map string booleans ('true'/'false') to 1/0.\"\"\"\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == bool:\n",
    "            df[c] = df[c].astype(int)\n",
    "        elif df[c].dtype == object:\n",
    "            vals = set(df[c].dropna().astype(str).str.lower().unique())\n",
    "            if vals <= {\"true\", \"false\"}:\n",
    "                df[c] = (\n",
    "                    df[c]\n",
    "                    .astype(str)\n",
    "                    .str.lower()\n",
    "                    .map({\"true\": 1, \"false\": 0})\n",
    "                )\n",
    "    return df\n",
    "\n",
    "\n",
    "def to_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Coerce all columns to numeric, setting non-convertible values to NaN.\"\"\"\n",
    "    return df.apply(lambda col: pd.to_numeric(col, errors=\"coerce\"))\n",
    "\n",
    "\n",
    "def put_label_first(df: pd.DataFrame, label: str = \"target\") -> pd.DataFrame:\n",
    "    \"\"\"Move the label column to be the first column if it exists.\"\"\"\n",
    "    if label in df.columns:\n",
    "        cols = [label] + [c for c in df.columns if c != label]\n",
    "        return df[cols]\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------- Main sanitization ----------\n",
    "def sanitize_one(name: str) -> None:\n",
    "    \"\"\"\n",
    "    Load {prefix}{name}.csv (with header) from S3, clean it, ensure binary label in col 0,\n",
    "    then upload back to the same key WITHOUT header. Print a short summary.\n",
    "    \"\"\"\n",
    "    key = f\"{prefix}{name}.csv\"\n",
    "\n",
    "    # Load with header, then clean\n",
    "    df = load_csv_with_header(key)\n",
    "    df = drop_dot1(df)\n",
    "    df = bool_to_int(df)\n",
    "    df = to_numeric(df).fillna(0.0)\n",
    "\n",
    "    # Put label first\n",
    "    df = put_label_first(df, \"target\")\n",
    "\n",
    "    # Force the first column (label) to be strictly 0/1\n",
    "    y = df.iloc[:, 0].values\n",
    "    uy = np.unique(y[~pd.isna(y)])\n",
    "\n",
    "    if not set(uy) <= {0, 1}:\n",
    "        if len(uy) == 2:\n",
    "            lo, hi = sorted(uy)\n",
    "            df.iloc[:, 0] = (df.iloc[:, 0] == hi).astype(int)\n",
    "        else:\n",
    "            thr = np.nanmedian(y)\n",
    "            df.iloc[:, 0] = (df.iloc[:, 0] > thr).astype(int)\n",
    "\n",
    "    # Upload back WI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the built-in Linear Learner image\n",
    "container = image_uris.retrieve(framework=\"linear-learner\", region=region)\n",
    "\n",
    "# Configure the estimator\n",
    "est = sagemaker.estimator.Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/{prefix}output/\",\n",
    "    sagemaker_session=sess,\n",
    "    max_run=3600,\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "est.set_hyperparameters(\n",
    "    predictor_type=\"binary_classifier\",\n",
    "    epochs=10,\n",
    "    mini_batch_size=256,\n",
    "    num_models=32,\n",
    "    loss=\"auto\",\n",
    ")\n",
    "\n",
    "# Training inputs\n",
    "train_input = TrainingInput(f\"s3://{bucket}/{prefix}train.csv\", content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(f\"s3://{bucket}/{prefix}val.csv\",   content_type=\"text/csv\")\n",
    "\n",
    "# Train\n",
    "est.fit({\"train\": train_input, \"validation\": val_input}, logs=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Deploy real-time endpoint ---\n",
    "predictor = est.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "endpoint_name = predictor.endpoint_name\n",
    "print(\"endpoint:\", endpoint_name)\n",
    "\n",
    "# --- Load test set (label + features) from S3 ---\n",
    "obj = s3.get_object(Bucket=bucket, Key=f\"{prefix}test.csv\")\n",
    "test_df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()), header=None)\n",
    "y_true = test_df.iloc[:, 0].astype(int).values\n",
    "X = test_df.iloc[:, 1:]\n",
    "\n",
    "# --- Upload features-only file for batch transform ---\n",
    "local_x = \"/tmp/test_x.csv\"\n",
    "X.to_csv(local_x, header=False, index=False)\n",
    "s3.upload_file(local_x, bucket, f\"{prefix}test_x.csv\")\n",
    "test_x_s3 = f\"s3://{bucket}/{prefix}test_x.csv\"\n",
    "\n",
    "# --- Configure and run Batch Transform ---\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "transformer = est.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/{prefix}batch_out/\",\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"application/jsonlines\",\n",
    ")\n",
    "transformer.transform(data=test_x_s3, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# --- Collect Batch Transform outputs from S3 ---\n",
    "s3r = boto3.resource(\"s3\")\n",
    "bkt = s3r.Bucket(bucket)\n",
    "outs = [o.key for o in bkt.objects.filter(Prefix=f\"{prefix}batch_out/\") if o.key.endswith(\".out\")]\n",
    "\n",
    "# --- Parse predictions ---\n",
    "y_pred, y_prob = [], []\n",
    "for key in outs:\n",
    "    body = bkt.Object(key).get()[\"Body\"].read().decode(\"utf-8\").strip()\n",
    "    if not body:\n",
    "        continue\n",
    "    for line in body.splitlines():\n",
    "        rec = json.loads(line)\n",
    "        prob = rec.get(\"score\", rec.get(\"scores\", [None])[0])\n",
    "        lab = rec.get(\"predicted_label\", int(prob >= 0.5) if prob is not None else 0)\n",
    "        y_prob.append(prob if prob is not None else float(lab))\n",
    "        y_pred.append(int(lab))\n",
    "\n",
    "y_prob = np.array(y_prob, dtype=float)\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "assert len(y_true) == len(y_pred) == len(y_prob), \"length mismatch\"\n",
    "\n",
    "# --- Metrics ---\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "try:\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "except ValueError:\n",
    "    auc = float(\"nan\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# --- Output (unchanged) ---\n",
    "print(\"Test metrics via (Batch Transform)\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "print(f\"ROC AUC  : {auc:.4f}\")\n",
    "print(\"Confusion matrix [[TN FP]\\n [FN TP]]:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Build and evaluate ensembe models\n",
    "\n",
    "Write code to perform the follwoing steps:\n",
    "1. Split data into training, validation and testing sets (70% - 15% - 15%).\n",
    "2. Use xgboost estimator to build a classifcation model.\n",
    "3. Host the model on another instance\n",
    "4. Perform batch transform to evaluate the model on testing data\n",
    "5. Report the performance metrics that you see better test the model performance \n",
    "6. write down your observation on the difference between the performance of using the simple and ensemble models.\n",
    "Note: You are required to perform the above steps on the two combined datasets separatey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the XGBoost container image\n",
    "container = image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.5-1\",\n",
    ")\n",
    "\n",
    "# Define the estimator\n",
    "xgb_est = sagemaker.estimator.Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/{prefix}xgb_output/\",\n",
    "    sagemaker_session=sess,\n",
    "    max_run=1800,\n",
    ")\n",
    "\n",
    "# Hyperparameters for binary classification\n",
    "xgb_est.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    num_round=50,\n",
    "    max_depth=4,\n",
    "    eta=0.2,\n",
    "    subsample=0.8,\n",
    "    eval_metric=\"logloss\",\n",
    ")\n",
    "\n",
    "# Training and validation inputs\n",
    "train_input = TrainingInput(f\"s3://{bucket}/{prefix}train.csv\", content_type=\"text/csv\")\n",
    "val_input   = TrainingInput(f\"s3://{bucket}/{prefix}val.csv\",   content_type=\"text/csv\")\n",
    "\n",
    "# Launch training\n",
    "xgb_est.fit({\"train\": train_input, \"validation\": val_input})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Deploy XGBoost real-time endpoint ---\n",
    "xgb_predictor = xgb_est.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    ")\n",
    "endpoint_name = xgb_predictor.endpoint_name\n",
    "print(\"Deployed XGBoost endpoint:\", endpoint_name)\n",
    "\n",
    "# --- Configure Batch Transform for XGBoost ---\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "transformer = xgb_est.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/{prefix}xgb_batch_out/\",\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"application/jsonlines\",\n",
    ")\n",
    "\n",
    "# --- Run Batch Transform on test features (CSV without header) ---\n",
    "test_x_s3 = f\"s3://{bucket}/{prefix}test_x.csv\"\n",
    "transformer.transform(data=test_x_s3, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import io\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# --- S3 resources ---\n",
    "s3r = boto3.resource(\"s3\")\n",
    "bkt = s3r.Bucket(bucket)\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Collect all .out shards from XGBoost Batch Transform (sorted for determinism)\n",
    "outs = sorted(\n",
    "    [\n",
    "        o.key\n",
    "        for o in bkt.objects.filter(Prefix=f\"{prefix}xgb_batch_out/\")\n",
    "        if o.key.endswith(\".out\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "def parse_prob(line: str):\n",
    "    \"\"\"\n",
    "    Extract a float probability from a batch-transform output line.\n",
    "    Handles JSON (scalar/dict/list), CSV, or whitespace-separated formats.\n",
    "    Returns None if no numeric value is found.\n",
    "    \"\"\"\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    # 1) Try JSON first\n",
    "    try:\n",
    "        obj = json.loads(line)\n",
    "\n",
    "        # JSON scalar\n",
    "        if isinstance(obj, (int, float)):\n",
    "            return float(obj)\n",
    "\n",
    "        # JSON dict\n",
    "        if isinstance(obj, dict):\n",
    "            # common keys for probability\n",
    "            for k in (\"score\", \"probability\", \"prediction\", \"predicted_value\"):\n",
    "                v = obj.get(k)\n",
    "                if isinstance(v, (int, float)):\n",
    "                    return float(v)\n",
    "            # list-bearing keys\n",
    "            for k in (\"scores\", \"predictions\"):\n",
    "                v = obj.get(k)\n",
    "                if isinstance(v, list) and v and isinstance(v[0], (int, float)):\n",
    "                    return float(v[0])\n",
    "            # generic: first numeric value\n",
    "            for v in obj.values():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    return float(v)\n",
    "\n",
    "        # JSON list\n",
    "        if isinstance(obj, list):\n",
    "            for v in obj:\n",
    "                if isinstance(v, (int, float)):\n",
    "                    return float(v)\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # 2) Try CSV / whitespace\n",
    "    for token in filter(None, re.split(r\"[,\\s]+\", line)):\n",
    "        try:\n",
    "            return float(token)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return None\n",
    "\n",
    "# --- Parse predictions from shards ---\n",
    "y_pred, y_prob = [], []\n",
    "for key in outs:\n",
    "    body = bkt.Object(key).get()[\"Body\"].read().decode(\"utf-8\")\n",
    "    for line in body.splitlines():\n",
    "        prob = parse_prob(line)\n",
    "        if prob is None:  # skip empty/garbage lines safely\n",
    "            continue\n",
    "        y_prob.append(prob)\n",
    "        y_pred.append(int(prob >= 0.5))\n",
    "\n",
    "y_prob = pd.Series(y_prob, dtype=\"float64\").to_numpy()\n",
    "y_pred = pd.Series(y_pred, dtype=\"int64\").to_numpy()\n",
    "\n",
    "# --- Load ground-truth labels (first column is label) ---\n",
    "obj = s3.get_object(Bucket=bucket, Key=f\"{prefix}test.csv\")\n",
    "test_df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()), header=None)\n",
    "y_true = test_df.iloc[:, 0].astype(int).to_numpy()\n",
    "\n",
    "# --- Sanity check: length match ---\n",
    "if len(y_true) != len(y_pred):\n",
    "    print(\n",
    "        f\"warning: length mismatch — labels={len(y_true)}, preds={len(y_pred)}. \"\n",
    "        f\"Did batch split your test into multiple shards or include a header?\"\n",
    "    )\n",
    "    n = min(len(y_true), len(y_pred))\n",
    "    y_true, y_pred, y_prob = y_true[:n], y_pred[:n], y_prob[:n]\n",
    "\n",
    "# --- Metrics ---\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "try:\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "except ValueError:\n",
    "    auc = float(\"nan\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# --- Output (unchanged) ---\n",
    "print(\"Test metrics via (XGBoost Batch Transform)\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "print(f\"ROC AUC  : {auc:.4f}\")\n",
    "print(\"Confusion matrix [[TN FP]\\n [FN TP]]:\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
